{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949715a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "ENDSWITH = 'OCR'\n",
    "\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "\n",
    "if not NOTEBOOK_DIR.endswith(ENDSWITH):\n",
    "    raise ValueError(f\"Not in correct dir, expect end with {ENDSWITH}, but got {NOTEBOOK_DIR} instead\")\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(NOTEBOOK_DIR, '..', '..', '..', '..'))\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "\n",
    "sys.path.insert(0, os.path.join(BASE_DIR, 'code'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e4d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MangaOCREvaluator import ParseAnnotation, MangaOCRDataset, MangaOCREvaluator\n",
    "from pipeline.OCRModels.MangaOCRModel import MangaOCRModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b66417",
   "metadata": {},
   "source": [
    "## Step 1: Parse XML Annotations\n",
    "\n",
    "Parse Manga109 XML annotations and save to JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a9416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "manga_name = \"AisazuNihaIrarenai\"\n",
    "xml_path = os.path.join(BASE_DIR, 'data', 'Manga109_released_2023_12_07', 'annotations', f'{manga_name}.xml')\n",
    "images_dir = os.path.join(BASE_DIR, 'data', 'Manga109_released_2023_12_07', 'images', manga_name)\n",
    "output_dir = os.path.join(BASE_DIR, 'data', 'MangaOCR', 'jsons_processed')\n",
    "\n",
    "# Parse annotations\n",
    "parser = ParseAnnotation(xml_path, images_dir, output_dir)\n",
    "json_output_path = parser.parse_and_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d957b6a",
   "metadata": {},
   "source": [
    "## Step 2: Load and Inspect Data\n",
    "\n",
    "Load the parsed annotations and check what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with text bbox\n",
    "text_data = MangaOCREvaluator.load_manga109_annotations(\n",
    "    json_output_path, \n",
    "    images_dir, \n",
    "    bbox_type=\"text\"\n",
    ")\n",
    "\n",
    "print(f\"Number of images with text: {len(text_data['image_paths'])}\")\n",
    "print(f\"\\nFirst image: {text_data['image_paths'][0]}\")\n",
    "print(f\"Number of text boxes: {len(text_data['boxes_list'][0])}\")\n",
    "print(f\"Text boxes: {text_data['boxes_list'][0][:3]}\")  # Show first 3\n",
    "print(f\"Ground truth: {text_data['ground_truth_texts'][0][:3]}\")  # Show first 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e0449",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate with Text BBox Only\n",
    "\n",
    "Evaluate OCR performance using only text bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56faea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with text bbox\n",
    "text_dataset = MangaOCRDataset(\n",
    "    text_data[\"image_paths\"][:5],  # Use first 5 images for demo\n",
    "    text_data[\"boxes_list\"][:5],\n",
    "    text_data[\"ground_truth_texts\"][:5],\n",
    "    bbox_type=\"text\"\n",
    ")\n",
    "\n",
    "# Initialize OCR model and evaluator\n",
    "ocr_model = MangaOCRModel()\n",
    "evaluator = MangaOCREvaluator(device=device)\n",
    "\n",
    "# Evaluate\n",
    "text_metrics = evaluator.evaluate(ocr_model, text_dataset, batch_size=1, verbose=False, bbox_type=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ea40e8",
   "metadata": {},
   "source": [
    "## Step 4: Compare Text BBox vs Bubble BBox\n",
    "\n",
    "Use the built-in comparison method to evaluate both bbox types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both bbox types\n",
    "ocr_model = MangaOCRModel()\n",
    "evaluator = MangaOCREvaluator(device=device)\n",
    "\n",
    "comparison_results = evaluator.compare_bbox_types(\n",
    "    ocr_model=ocr_model,\n",
    "    json_path=json_output_path,\n",
    "    images_dir=images_dir,\n",
    "    batch_size=1,\n",
    "    verbose=False,\n",
    "    max_images=5  # Limit to first 5 images for demo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5fb1ad",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Results\n",
    "\n",
    "The comparison shows:\n",
    "- **Text BBox**: Uses the exact text region bounding boxes from annotations\n",
    "- **Bubble BBox**: Uses speech bubble bounding boxes (if available)\n",
    "\n",
    "Key insights:\n",
    "- Lower CER/WER is better\n",
    "- Text bbox should theoretically perform better as it's more precise\n",
    "- Bubble bbox includes extra whitespace/background which may affect OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a03555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access individual results\n",
    "print(\"Text BBox Results:\")\n",
    "print(f\"  CER: {comparison_results['text_bbox']['cer']:.4f}\")\n",
    "print(f\"  WER: {comparison_results['text_bbox']['wer']:.4f}\")\n",
    "print(f\"  Samples: {comparison_results['text_bbox']['num_samples']}\")\n",
    "\n",
    "print(\"\\nBubble BBox Results:\")\n",
    "print(f\"  CER: {comparison_results['bubble_bbox']['cer']:.4f}\")\n",
    "print(f\"  WER: {comparison_results['bubble_bbox']['wer']:.4f}\")\n",
    "print(f\"  Samples: {comparison_results['bubble_bbox']['num_samples']}\")\n",
    "\n",
    "# Calculate improvement\n",
    "cer_diff = comparison_results['text_bbox']['cer'] - comparison_results['bubble_bbox']['cer']\n",
    "wer_diff = comparison_results['text_bbox']['wer'] - comparison_results['bubble_bbox']['wer']\n",
    "\n",
    "print(f\"\\nImprovement using Text BBox:\")\n",
    "print(f\"  CER: {cer_diff:.4f} ({'better' if cer_diff < 0 else 'worse'})\")\n",
    "print(f\"  WER: {wer_diff:.4f} ({'better' if wer_diff < 0 else 'worse'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e647f",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### ParseAnnotation Class\n",
    "- Input: Manga109 XML annotation file\n",
    "- Output: JSON file in COCO format with text annotations\n",
    "- Saves to: `data/MangaOCR/jsons_processed/`\n",
    "\n",
    "### MangaOCRDataset Class\n",
    "- No longer depends on YoloSeg output\n",
    "- Works directly with parsed JSON annotations\n",
    "- Loads images on-the-fly to save memory\n",
    "- Supports both \"text\" and \"bubble\" bbox types\n",
    "\n",
    "### MangaOCREvaluator\n",
    "- `load_manga109_annotations()`: Loads and filters data from JSON\n",
    "- `evaluate()`: Evaluates OCR with specified bbox type\n",
    "- `compare_bbox_types()`: Automatically compares text vs bubble bboxes\n",
    "\n",
    "### Workflow\n",
    "1. Parse XML â†’ JSON (one-time setup)\n",
    "2. Load JSON annotations\n",
    "3. Create dataset with desired bbox type\n",
    "4. Evaluate OCR performance\n",
    "5. Compare results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
