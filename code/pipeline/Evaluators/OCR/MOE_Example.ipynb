{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9103bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "ENDSWITH = 'OCR'\n",
    "\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "\n",
    "if not NOTEBOOK_DIR.endswith(ENDSWITH):\n",
    "    raise ValueError(f\"Not in correct dir, expect end with {ENDSWITH}, but got {NOTEBOOK_DIR} instead\")\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(NOTEBOOK_DIR, '..', '..', '..', '..'))\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "\n",
    "sys.path.insert(0, os.path.join(BASE_DIR, 'code'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ee81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a51b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MangaOCREvaluator import MangaOCRDataset, MangaOCREvaluator\n",
    "from pipeline.SegmentationModels.YoloSeg import YoloSeg\n",
    "from pipeline.OCRModels.MangaOCRModel import MangaOCRModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c3d6ae",
   "metadata": {},
   "source": [
    "## Example 1: Evaluate OCR on a Single Image\n",
    "\n",
    "This example shows how to:\n",
    "1. Use YoloSeg to detect text bubbles\n",
    "2. Create a dataset from the detection results\n",
    "3. Evaluate OCR performance with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the segmentation model\n",
    "yolo_model_path = os.path.join(BASE_DIR, 'models', 'bubble-detection', 'best.pt')\n",
    "yolo_seg = YoloSeg(yolo_model_path)\n",
    "yolo_seg.load_model()\n",
    "\n",
    "# Process a sample image\n",
    "sample_image_path = os.path.join(BASE_DIR, 'data', 'Manga109_released_2023_12_07', 'images', 'AisazuNihaIrarenai', '002.jpg')\n",
    "\n",
    "# Get segmentation results\n",
    "img_rgb, boxes, masks = yolo_seg.predict(sample_image_path, print_bbox=False, plot=False)\n",
    "\n",
    "print(f\"Detected {len(boxes)} text bubbles\")\n",
    "yolo_seg.unload_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a489d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example ground truth texts (in real scenario, load from annotations)\n",
    "# For demonstration, we'll create dummy ground truth\n",
    "ground_truth_texts = [\"Sample text \" + str(i+1) for i in range(len(boxes))]\n",
    "\n",
    "# Create dataset\n",
    "dataset = MangaOCRDataset(\n",
    "    images=[img_rgb],\n",
    "    boxes_list=[boxes],\n",
    "    masks_list=[masks],\n",
    "    ground_truth_texts=[ground_truth_texts]\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} image(s)\")\n",
    "print(f\"First image has {len(boxes)} text bubbles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dfc445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OCR model and evaluator\n",
    "ocr_model = MangaOCRModel()\n",
    "evaluator = MangaOCREvaluator(device=device)\n",
    "\n",
    "# Evaluate\n",
    "metrics = evaluator.evaluate(ocr_model, dataset, batch_size=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b5cc6c",
   "metadata": {},
   "source": [
    "## Example 2: Batch Evaluation on Multiple Images\n",
    "\n",
    "This example demonstrates batch processing of multiple manga pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c158959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple images\n",
    "yolo_seg = YoloSeg(yolo_model_path)\n",
    "yolo_seg.load_model()\n",
    "\n",
    "image_dir = os.path.join(BASE_DIR, 'data', 'Manga109_released_2023_12_07', 'images', 'AisazuNihaIrarenai')\n",
    "image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')][:3]  # First 3 images\n",
    "\n",
    "images = []\n",
    "boxes_list = []\n",
    "masks_list = []\n",
    "ground_truth_list = []\n",
    "\n",
    "for img_file in image_files:\n",
    "    img_path = os.path.join(image_dir, img_file)\n",
    "    img_rgb, boxes, masks = yolo_seg.predict(img_path, print_bbox=False, plot=False)\n",
    "    \n",
    "    images.append(img_rgb)\n",
    "    boxes_list.append(boxes)\n",
    "    masks_list.append(masks)\n",
    "    \n",
    "    # Create dummy ground truth (replace with actual annotations in production)\n",
    "    gt_texts = [f\"Text from {img_file} box {i+1}\" for i in range(len(boxes))]\n",
    "    ground_truth_list.append(gt_texts)\n",
    "    \n",
    "    print(f\"Processed {img_file}: {len(boxes)} bubbles detected\")\n",
    "\n",
    "yolo_seg.unload_model()\n",
    "\n",
    "# Create dataset\n",
    "batch_dataset = MangaOCRDataset(images, boxes_list, masks_list, ground_truth_list)\n",
    "print(f\"\\nBatch dataset created with {len(batch_dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on batch\n",
    "ocr_model = MangaOCRModel()\n",
    "evaluator = MangaOCREvaluator(device=device)\n",
    "\n",
    "batch_metrics = evaluator.evaluate(ocr_model, batch_dataset, batch_size=1, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6fec3f",
   "metadata": {},
   "source": [
    "## Example 3: Using Real Manga109Dialog Annotations\n",
    "\n",
    "This example shows how to load real ground truth from XML annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784b5540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations for a specific manga\n",
    "xml_path = os.path.join(BASE_DIR, 'data', 'Manga109Dialog', 'AisazuNihaIrarenai.xml')\n",
    "\n",
    "# Load text annotations\n",
    "text_dict = MangaOCREvaluator.load_manga109_annotations(xml_path)\n",
    "print(f\"Loaded {len(text_dict)} text annotations from XML\")\n",
    "print(f\"Sample text IDs: {list(text_dict.keys())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c567b03a",
   "metadata": {},
   "source": [
    "## Notes on Usage\n",
    "\n",
    "### Input Format\n",
    "- **images**: List of numpy arrays (RGB format)\n",
    "- **boxes_list**: List of lists, each containing bounding boxes [[x_min, y_min, x_max, y_max], ...]\n",
    "- **masks_list**: List of masks from YoloSeg output\n",
    "- **ground_truth_texts**: List of lists, each containing text strings for corresponding boxes\n",
    "\n",
    "### Metrics\n",
    "- **CER (Character Error Rate)**: Measures character-level accuracy\n",
    "- **WER (Word Error Rate)**: Measures word-level accuracy\n",
    "\n",
    "### Best Practices\n",
    "1. Use batch_size=1 for manga images to avoid memory issues\n",
    "2. Filter empty ground truth texts before evaluation\n",
    "3. Load and unload models properly to manage memory\n",
    "4. Use verbose=True for debugging individual predictions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
