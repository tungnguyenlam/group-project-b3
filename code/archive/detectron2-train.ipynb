{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ae8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls\n",
    "%cd ../..\n",
    "!ls\n",
    "# Now you should see the bubble-segmentation-final-deep-learning directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac19a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "END_WITH_LOCAL = 'bubble-segmentation-final-deep-learning'\n",
    "\n",
    "os.environ['PATH'] = f\"/root/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "print(f\"BASE_DIR: {BASE_DIR}\")\n",
    "\n",
    "# Simple validation\n",
    "if not (BASE_DIR.endswith('/content') or BASE_DIR.endswith(END_WITH_LOCAL)):\n",
    "    raise ValueError(f\"Expected to be in .../{END_WITH_LOCAL} or .../content directory, but got: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d5585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "base_path = os.path.join(BASE_DIR, \"data\", \"Manga109_released_2023_12_07\")  # Adjusted to the new dataset directory\n",
    "annotations_dir = os.path.join(base_path, \"annotations\")\n",
    "CATEGORIES_TO_DETECT = [\"body\", \"frame\"]\n",
    "VAL_SPLIT_RATIO = 0.2  # Use 20% of the data for validation\n",
    "\n",
    "def process_books(book_list, output_filename):\n",
    "    \"\"\"Processes a list of books and saves them to a COCO JSON file.\"\"\"\n",
    "    \n",
    "    # Category mapping (IDs should start from 1)\n",
    "    categories_coco = [{\"id\": i, \"name\": name} for i, name in enumerate(CATEGORIES_TO_DETECT, start=1)]\n",
    "    category_map = {name: i for i, name in enumerate(CATEGORIES_TO_DETECT, start=1)}\n",
    "    \n",
    "    coco_output = {\n",
    "        \"info\": {\n",
    "            \"description\": \"Manga109 Dataset for Detectron2\",\n",
    "            \"version\": \"1.0\",\n",
    "        },\n",
    "        \"licenses\": [],\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": categories_coco\n",
    "    }\n",
    "    \n",
    "    annotation_id_counter = 1\n",
    "    image_id_counter = 1\n",
    "    \n",
    "    print(f\"Processing {len(book_list)} books for '{output_filename}'...\")\n",
    "    for book in tqdm(book_list, desc=f\"Processing for {output_filename}\"):\n",
    "        xml_file_path = os.path.join(annotations_dir, f\"{book}.xml\")\n",
    "        \n",
    "        tree = ET.parse(xml_file_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        for page in root.findall(\".//page\"):\n",
    "            page_index = page.attrib['index']\n",
    "            image_width = int(page.attrib['width'])\n",
    "            image_height = int(page.attrib['height'])\n",
    "            \n",
    "            image_rel_path = os.path.join(book, f\"{page_index.zfill(3)}.jpg\")\n",
    "            \n",
    "            image_info = {\n",
    "                \"id\": image_id_counter,\n",
    "                \"file_name\": image_rel_path,\n",
    "                \"width\": image_width,\n",
    "                \"height\": image_height,\n",
    "            }\n",
    "            coco_output[\"images\"].append(image_info)\n",
    "\n",
    "            for category_name in CATEGORIES_TO_DETECT:\n",
    "                for obj in page.findall(category_name):\n",
    "                    bbox = obj.attrib\n",
    "                    xmin = int(bbox['xmin'])\n",
    "                    ymin = int(bbox['ymin'])\n",
    "                    width = int(bbox['xmax']) - xmin\n",
    "                    height = int(bbox['ymax']) - ymin\n",
    "                    \n",
    "                    annotation_info = {\n",
    "                        \"id\": annotation_id_counter,\n",
    "                        \"image_id\": image_id_counter,\n",
    "                        \"category_id\": category_map[category_name],\n",
    "                        \"bbox\": [xmin, ymin, width, height],\n",
    "                        \"area\": width * height,\n",
    "                        \"iscrowd\": 0,\n",
    "                    }\n",
    "                    coco_output[\"annotations\"].append(annotation_info)\n",
    "                    annotation_id_counter += 1\n",
    "            \n",
    "            image_id_counter += 1\n",
    "\n",
    "    output_json_path = os.path.join(base_path, output_filename)\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(coco_output, f)\n",
    "        \n",
    "    print(f\"\\nConversion complete! File saved to: {output_json_path}\")\n",
    "    return output_json_path\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    all_books = [f.replace('.xml', '') for f in os.listdir(annotations_dir) if f.endswith('.xml')]\n",
    "    random.shuffle(all_books) # Shuffle for a random split\n",
    "\n",
    "    split_index = int(len(all_books) * VAL_SPLIT_RATIO)\n",
    "    val_books = all_books[:split_index]\n",
    "    train_books = all_books[split_index:]\n",
    "    \n",
    "    print(f\"Total books: {len(all_books)}. Training books: {len(train_books)}. Validation books: {len(val_books)}.\")\n",
    "\n",
    "    # Process and save the training and validation sets\n",
    "    process_books(train_books, \"train.json\")\n",
    "    process_books(val_books, \"val.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "import random\n",
    "import cv2\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Paths ---\n",
    "base_path = os.path.join(BASE_DIR, \"data\", \"Manga109_released_2023_12_07\")  # Adjusted to the new dataset directory\n",
    "images_dir = os.path.join(base_path, \"images\")\n",
    "train_json_path = os.path.join(base_path, \"train.json\")\n",
    "val_json_path = os.path.join(base_path, \"val.json\")\n",
    "CATEGORIES_TO_DETECT = [\"body\", \"frame\"]\n",
    "\n",
    "# --- Register the dataset ---\n",
    "# The first argument is the name you will use to refer to this dataset.\n",
    "# Register the training dataset\n",
    "register_coco_instances(\"manga109_train\", {}, train_json_path, images_dir)\n",
    "\n",
    "# Register the validation dataset\n",
    "register_coco_instances(\"manga109_val\", {}, val_json_path, images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29484bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "# --- Configuration Setup ---\n",
    "cfg = get_cfg()\n",
    "\n",
    "# Start with a pre-trained model. Faster R-CNN is a good choice.\n",
    "# Find more models in the Detectron2 Model Zoo: https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "# cfg.MODEL.WEIGHTS = os.path.join(BASE_DIR, \"models/bubble-detection/detectron2/model_final_280758.pkl\")\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "\n",
    "cfg.MODEL.DEVICE = str(device)\n",
    "\n",
    "# --- Dataset Configuration ---\n",
    "cfg.DATASETS.TRAIN = (\"manga109_train\",)\n",
    "# --- ADD THE TEST DATASET ---\n",
    "cfg.DATASETS.TEST = (\"manga109_val\",) \n",
    "# ----------------------------\n",
    "\n",
    "# --- Evaluation Configuration ---\n",
    "# How often to run evaluation (in number of iterations).\n",
    "cfg.TEST.EVAL_PERIOD = 500 \n",
    "# --------------------------------\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 2 # 12\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  # body, frame\n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2 # 8\n",
    "cfg.SOLVER.BASE_LR = 0.0025\n",
    "cfg.SOLVER.MAX_ITER = 3000 \n",
    "cfg.SOLVER.STEPS = []\n",
    "\n",
    "cfg.OUTPUT_DIR = os.path.join(BASE_DIR, \"models\", \"bubble-detection\",\"detectron2\")\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Start Training ---\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "\n",
    "# --- Setup the predictor ---\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # Path to your trained model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # Set a threshold for predictions\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Get the metadata and the list of data dictionaries for the validation set\n",
    "val_metadata = MetadataCatalog.get(\"manga109_val\")\n",
    "dataset_dicts = DatasetCatalog.get(\"manga109_val\")\n",
    "\n",
    "# Randomly select a few images from the validation set to visualize\n",
    "num_images_to_show = 3\n",
    "for d in random.sample(dataset_dicts, num_images_to_show):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    if im is None:\n",
    "        print(f\"Warning: Could not read image {d['file_name']}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Make predictions\n",
    "    outputs = predictor(im)\n",
    "\n",
    "    # --- Visualize Predictions ---\n",
    "    v_pred = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=val_metadata, \n",
    "                   scale=0.8,\n",
    "                   instance_mode=ColorMode.IMAGE_BW # or ColorMode.IMAGE\n",
    "    )\n",
    "    predictions_vis = v_pred.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "    # --- Visualize Ground Truth ---\n",
    "    v_gt = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=val_metadata, \n",
    "                   scale=0.8\n",
    "    )\n",
    "    ground_truth_vis = v_gt.draw_dataset_dict(d)\n",
    "\n",
    "    # --- Display side-by-side ---\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    \n",
    "    ax1.imshow(predictions_vis.get_image()[:, :, ::-1])\n",
    "    ax1.set_title(\"Model Predictions\")\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    ax2.imshow(ground_truth_vis.get_image()[:, :, ::-1])\n",
    "    ax2.set_title(\"Ground Truth\")\n",
    "    ax2.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
