{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f61b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /Users/tungnguyen/Library/CloudStorage/GoogleDrive-nguyenlamtungthptltt@gmail.com/My Drive/Projects-Large/Group-Project-B3/code-just-to-preview/characters-and-dialouges-association-in-comics/code/bubble-detection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "END_WITH_LOCAL = 'archive'\n",
    "\n",
    "os.environ['PATH'] = f\"/root/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "print(f\"BASE_DIR: {BASE_DIR}\")\n",
    "\n",
    "# Simple validation\n",
    "if not (BASE_DIR.endswith('/content') or BASE_DIR.endswith(END_WITH_LOCAL)):\n",
    "    raise ValueError(f\"Expected to be in .../{END_WITH_LOCAL} or .../content directory, but got: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ea9a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "Manga109_dir = os.path.join(BASE_DIR,'../../data/Manga109/Manga109/Manga109_released_2023_12_07/Manga109_released_2023_12_07/images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "471dd50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NichijouSoup', 'GakuenNoise', 'TsubasaNoKioku', 'JangiriPonpon', 'That_sIzumiko', 'BokuHaSitatakaKun', 'GinNoChimera', 'GarakutayaManta', 'MagicStarGakuin', 'UchiNoNyan_sDiary', 'ToutaMairimasu', 'Akuhamu', 'EvaLady', 'UltraEleven', 'Nekodama', 'OhWareraRettouSeitokai', 'HighschoolKimengumi_vol20', 'YamatoNoHane', 'YouchienBoueigumi', 'AppareKappore', 'HeiseiJimen', 'MukoukizuNoChonbo', 'HanzaiKousyouninMinegishiEitarou', 'SaladDays_vol01', 'UnbalanceTokyo', 'TotteokiNoABC', 'HaruichibanNoFukukoro', 'GOOD_KISS_Ver2', 'PlatinumJungle', 'SyabondamaKieta', 'PrayerHaNemurenai', 'MoeruOnisan_vol19', 'SamayoeruSyonenNiJunaiWo', 'TapkunNoTanteisitsu', 'JijiBabaFight', 'MayaNoAkaiKutsu', 'AosugiruHaru', 'ByebyeC-BOY', 'PrismHeart', 'Donburakokko', 'HinagikuKenzan', 'EienNoWith', 'HisokaReturns', 'BEMADER_P', 'MeteoSanStrikeDesu', 'Saisoku', 'TasogareTsushin', 'WarewareHaOniDearu', 'ShimatteIkouze_vol01', 'YumeiroCooking', 'UchuKigekiM774', 'TetsuSan', 'ParaisoRoad', 'Joouari', 'YumeNoKayoiji', 'HealingPlanet', 'BurariTessenTorimonocho', 'MagicianLoad', 'PikaruGenkiDesu', 'KuroidoGanka', 'MiraiSan', 'MutekiBoukenSyakuma', 'ShimatteIkouze_vol26', 'MadouTaiga', 'PsychoStaff', 'YukiNoFuruMachi', 'TouyouKidan', 'BakuretsuKungFuGirl', 'MisutenaideDaisy', 'KyokugenCyclone', 'ReveryEarth', 'KoukouNoHitotachi', 'PLANET7', 'LoveHina_vol14', 'MoeruOnisan_vol01', 'DualJustice', 'HarukaRefrain', 'KimiHaBokuNoTaiyouDa', 'MariaSamaNihaNaisyo', 'YasasiiAkuma', 'DollGun', 'LoveHina_vol01', 'TensiNoHaneToAkumaNoShippo', 'MemorySeijin', 'TennenSenshiG', 'Ningyoushi', 'MAD_STONE', 'Hamlet', 'AkkeraKanjinchou', 'HighschoolKimengumi_vol01', 'Raphael', 'ARMS', 'SeisinkiVulnus', 'SaladDays_vol18', 'RinToSiteSippuNoNaka', 'KarappoHighschool', 'MomoyamaHaikagura', 'LancelotFullThrottle', 'Jyovolley', 'SonokiDeABC', 'Arisa', 'Belmondo', 'TaiyouNiSmash', 'YoumaKourin', 'OL_Lunch', 'EverydayOsakanaChan', 'Count3DeKimeteAgeru', 'AisazuNihaIrarenai', 'RisingGirl']\n"
     ]
    }
   ],
   "source": [
    "folders = [f.name for f in Path(Manga109_dir).iterdir() if f.is_dir()]\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6635b91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARMS', 'AisazuNihaIrarenai', 'AkkeraKanjinchou', 'Akuhamu', 'AosugiruHaru', 'AppareKappore', 'Arisa', 'BEMADER_P', 'BakuretsuKungFuGirl', 'Belmondo', 'BokuHaSitatakaKun', 'BurariTessenTorimonocho', 'ByebyeC-BOY', 'Count3DeKimeteAgeru', 'DollGun', 'Donburakokko', 'DualJustice', 'EienNoWith', 'EvaLady', 'EverydayOsakanaChan', 'GOOD_KISS_Ver2', 'GakuenNoise', 'GarakutayaManta', 'GinNoChimera', 'Hamlet', 'HanzaiKousyouninMinegishiEitarou', 'HaruichibanNoFukukoro', 'HarukaRefrain', 'HealingPlanet', 'HeiseiJimen', 'HighschoolKimengumi_vol01', 'HighschoolKimengumi_vol20', 'HinagikuKenzan', 'HisokaReturns', 'JangiriPonpon', 'JijiBabaFight', 'Joouari', 'Jyovolley', 'KarappoHighschool', 'KimiHaBokuNoTaiyouDa', 'KoukouNoHitotachi', 'KuroidoGanka', 'KyokugenCyclone', 'LancelotFullThrottle', 'LoveHina_vol01', 'LoveHina_vol14', 'MAD_STONE', 'MadouTaiga', 'MagicStarGakuin', 'MagicianLoad', 'MariaSamaNihaNaisyo', 'MayaNoAkaiKutsu', 'MemorySeijin', 'MeteoSanStrikeDesu', 'MiraiSan', 'MisutenaideDaisy', 'MoeruOnisan_vol01', 'MoeruOnisan_vol19', 'MomoyamaHaikagura', 'MukoukizuNoChonbo', 'MutekiBoukenSyakuma', 'Nekodama', 'NichijouSoup', 'Ningyoushi', 'OL_Lunch', 'OhWareraRettouSeitokai', 'PLANET7', 'ParaisoRoad', 'PikaruGenkiDesu', 'PlatinumJungle', 'PrayerHaNemurenai', 'PrismHeart', 'PsychoStaff', 'Raphael', 'ReveryEarth', 'RinToSiteSippuNoNaka', 'RisingGirl', 'Saisoku', 'SaladDays_vol01', 'SaladDays_vol18', 'SamayoeruSyonenNiJunaiWo', 'SeisinkiVulnus', 'ShimatteIkouze_vol01', 'ShimatteIkouze_vol26', 'SonokiDeABC', 'SyabondamaKieta', 'TaiyouNiSmash', 'TapkunNoTanteisitsu', 'TasogareTsushin', 'TennenSenshiG', 'TensiNoHaneToAkumaNoShippo', 'TetsuSan', 'That_sIzumiko', 'TotteokiNoABC', 'ToutaMairimasu', 'TouyouKidan', 'TsubasaNoKioku', 'UchiNoNyan_sDiary', 'UchuKigekiM774', 'UltraEleven', 'UnbalanceTokyo', 'WarewareHaOniDearu', 'YamatoNoHane', 'YasasiiAkuma', 'YouchienBoueigumi', 'YoumaKourin', 'YukiNoFuruMachi', 'YumeNoKayoiji', 'YumeiroCooking']\n"
     ]
    }
   ],
   "source": [
    "folders = sorted([f.name for f in Path(Manga109_dir).iterdir() if f.is_dir()])\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "718129ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "original_image_path = []\n",
    "\n",
    "# Get the first 30 sorted (case-sensitive) folders (volumes)\n",
    "first_30_volumes = sorted([f for f in Path(Manga109_dir).iterdir() if f.is_dir()], key=lambda x: x.name)[:30]\n",
    "\n",
    "# For each volume, get the first 11 images sorted in ascending order\n",
    "for volume in first_30_volumes:\n",
    "    images = sorted([f for f in volume.iterdir() if f.is_file() and f.suffix.lower() == '.jpg'], key=lambda x: x.name)\n",
    "    for img_path in images[:16]:\n",
    "        original_image_path.append(str(img_path))\n",
    "\n",
    "print(len(original_image_path))\n",
    "\n",
    "human_annotate_dir = os.path.join(BASE_DIR,'../../data/Human_Annotate_300/train')\n",
    "\n",
    "all_img_paths = []\n",
    "\n",
    "# for root, dirs, files in os.walk(human_annotate_dir):\n",
    "#     for file in files:\n",
    "#         if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "#             all_img_paths.append(os.path.join(root, file))\n",
    "\n",
    "# Only scan immediate directory (no subdirectories)\n",
    "all_img_paths = []\n",
    "for file in os.listdir(human_annotate_dir):\n",
    "    file_path = os.path.join(human_annotate_dir, file)\n",
    "    if os.path.isfile(file_path) and file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        all_img_paths.append(file_path)\n",
    "\n",
    "print(len(all_img_paths))\n",
    "\n",
    "for volume in first_30_volumes:\n",
    "    os.makedirs(os.path.join(human_annotate_dir, volume.name), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "566dc51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# def compare_images(path1, path2):\n",
    "#     img1 = cv2.imread(path1)\n",
    "#     img2 = cv2.imread(path2)\n",
    "\n",
    "#     if img1 is None or img2 is None:\n",
    "#         return False\n",
    "#     if img1.shape != img2.shape:\n",
    "#         return False\n",
    "#     if np.array_equal(img1, img2):\n",
    "#         plt.figure(figsize=(10, 5))\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "#         plt.title('Image Original to compare')\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "#         plt.title('Image in Human Annotate')\n",
    "#         plt.show()\n",
    "#         time.sleep(0.2)\n",
    "#         plt.close()\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "def compare_images(path1, path2, threshold=0.95):\n",
    "    img1 = cv2.imread(path1)\n",
    "    img2 = cv2.imread(path2)\n",
    "\n",
    "    if img1 is None or img2 is None:\n",
    "        return False\n",
    "    \n",
    "    # If shapes are different, try to resize to match\n",
    "    if img1.shape != img2.shape:\n",
    "        # Resize img2 to match img1's dimensions\n",
    "        img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]))\n",
    "    \n",
    "    # Method 1: Structural Similarity Index (SSIM)\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "    \n",
    "    # Convert to grayscale for SSIM\n",
    "    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    similarity_score = ssim(gray1, gray2)\n",
    "    \n",
    "    if similarity_score >= threshold:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Image Original to compare')\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Image in Human Annotate')\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.text(0.5, 0.5, f'SSIM Score: {similarity_score:.4f}', \n",
    "                 horizontalalignment='center', verticalalignment='center', \n",
    "                 transform=plt.gca().transAxes, fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        time.sleep(0.2)\n",
    "        plt.close()\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1ea0d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total original images to find: 448\n",
      "Total images in human annotate directory: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing original images: 100%|██████████| 448/448 [09:37<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "count_found = 0\n",
    "original_image_path_len = len(original_image_path)\n",
    "print(f\"Total original images to find: {original_image_path_len}\")\n",
    "all_img_paths_len = len(all_img_paths)\n",
    "print(f\"Total images in human annotate directory: {all_img_paths_len}\")\n",
    "\n",
    "for img_path in tqdm(original_image_path, desc=\"Processing original images\"):\n",
    "    copy_img_path = os.path.join(human_annotate_dir, Path(img_path).parent.name)\n",
    "    copy_img_name = Path(img_path).name\n",
    "    for img_human in all_img_paths:\n",
    "        if compare_images(img_path, img_human):\n",
    "            found = True\n",
    "            # Copy and rename the image\n",
    "            shutil.copy(img_human, os.path.join(copy_img_path, copy_img_name))\n",
    "            # Copy and rename the XML if it exists\n",
    "            xml_human = os.path.splitext(img_human)[0] + \".xml\"\n",
    "            if os.path.exists(xml_human):\n",
    "                shutil.copy(xml_human, os.path.join(copy_img_path, os.path.splitext(copy_img_name)[0] + \".xml\"))\n",
    "                os.remove(xml_human)  # Remove the xml file after copying\n",
    "            # Remove img_human from all_img_paths and delete the file\n",
    "            all_img_paths.remove(img_human)\n",
    "            os.remove(img_human)  # Remove the image file after copying\n",
    "            count_found += 1\n",
    "            break\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a87c2444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(count_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c9b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
